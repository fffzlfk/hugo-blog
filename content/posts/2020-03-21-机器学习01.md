---
author: fffzlfk
catalog: true
date: 2020-03-21
subtitle: 卷积神经网络入门 & 手写数字识别
tags:
  - 机器学习
title: 机器学习01
url: !!binary |
  LzIwMjAvMDMvMjEvu/rG99Gnz7AwMS8=
---


### 卷积神经网络入门
```python
import tensorflow as tf
import numpy as np
from tensorflow import keras

# 创建一层的神经网络
model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
# 指定loss和optimizer函数 'sgd'(梯度随机下降) (均方误差)
model.compile(optimizer='sgd', loss='mean_squared_error')

# 生成测试数据
X = []
for i in range(-1, 5):
    X.append(i * 1.0)

Y = [3 * i + 1 for i in X]

xs = np.array(X, dtype=float)
ys = np.array(Y, dtype=float)

model.fit(xs, ys, epochs=2500)

# 使用模型
print(model.predict([0.0]))
```

### 手写数字识别代码
```python
from tensorflow.examples.tutorials.mnist import input_data

minist = input_data.read_data_sets('/home/fffzlfk/Python/data', one_hot=True)

learning_rate = 0.01
batch_size = 125
n_epochs = 30


import tensorflow as tf

X = tf.placeholder(tf.float32, [batch_size, 784])
Y = tf.placeholder(tf.int32, [batch_size, 10])

with tf.name_scope('Wx_b') as scope:
    w = tf.Variable(tf.random_normal(
        shape=[784, 10], stddev=0.01), name='weights')
    b = tf.Variable(tf.zeros([1, 10]), name='bits')
    logits = tf.matmul(X, w) + b


with tf.name_scope('cost') as scope:
    entropy = tf.nn.softmax_cross_entropy_with_logits(
        logits=logits, labels=Y, name='loss')
    loss = tf.reduce_mean(entropy)
    tf.summary.scalar('loss', loss)


with tf.name_scope('train') as scope:
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)


summary = tf.summary.merge_all()

with tf.Session() as sess:
    writer = tf.summary.FileWriter(
        '/home/fffzlfk/Python/data/graphs', sess.graph)
    sess.run(tf.global_variables_initializer())
    n_batches = int(minist.train.num_examples / batch_size)

    for i in range(n_epochs):
        total_loss = 0
        for j in range(n_batches):
            X_batch, Y_batch = minist.train.next_batch(batch_size)
            _, loss_batch = sess.run([optimizer, loss], feed_dict={
                                     X: X_batch, Y: Y_batch})
        total_loss += loss_batch
        summary_str = sess.run(summary, feed_dict={X: X_batch, Y: Y_batch})
        writer.add_summary(summary_str, i * n_batches)
        print('Average loss epoch {0}: {1}'.format(i, total_loss / n_batches))
```